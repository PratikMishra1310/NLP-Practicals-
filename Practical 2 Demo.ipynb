{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d346ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural Language ToolKit\n",
    "import nltk\n",
    "\n",
    "paragraph =  \"\"\"Sabeena met Homai to shoot her documentary Three Women and a Camera in 1997, \n",
    "                and ended up uncovering a feminist history of Indian nation building rarely seen, never heard, \n",
    "                and lovingly told in the Camera Chronicles of Homai Vyarawalla. “Homai lived a quiet retired \n",
    "                life for 23 years and she took all the success in the same philosophical way as she took the anonymity,” \n",
    "                says Sabeena, eight years after Homai’s passing. “The film was only the beginning of a long association \n",
    "                with Homai. We wrote letters to each other, I would travel to Baroda every few month to look at her \n",
    "                photographs and to try and understand the person behind these images.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85caecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stopwords=words in (NLP) that are considered inconsequential and are filtered out before or after processing text.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4fda24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "#portstemmer =  is (NLP) algorithm that removes suffixes from English words to get their stems.\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#WordNetLemmatizer = links similar meaning words as one word\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44243b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e271e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\abhij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abhij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#Wordnet = large lexical database of English.\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "#punkt = is an unsupervised trainable model\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d347d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sabeena met homai shoot documentary three woman camera ended uncovering feminist history indian nation building rarely seen never heard lovingly told camera chronicle homai vyarawalla', 'homai lived quiet retired life year took success philosophical way took anonymity say sabeena eight year homai passing', 'film beginning long association homai', 'wrote letter would travel baroda every month look photograph try understand person behind image']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "#tokenizer = the process of converting a sequence of text into smaller parts, known as tokens.\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "# print(sentences)\n",
    "# print(len(sentences))\n",
    "#corpus = a collection of authentic texts or audio organized into datasets that represents a specific language or language variety.\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "#     review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "print(corpus)\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "777a8e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "a\n",
      "b\n",
      "e\n",
      "e\n",
      "n\n",
      "a\n",
      " \n",
      "m\n",
      "e\n",
      "t\n",
      " \n",
      "h\n",
      "o\n",
      "m\n",
      "a\n",
      "i\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "s\n",
      "h\n",
      "o\n",
      "o\n",
      "t\n",
      " \n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "d\n",
      "o\n",
      "c\n",
      "u\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      "a\n",
      "r\n",
      "y\n",
      " \n",
      "t\n",
      "h\n",
      "r\n",
      "e\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "m\n",
      "e\n",
      "n\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "a\n",
      " \n",
      "c\n",
      "a\n",
      "m\n",
      "e\n",
      "r\n",
      "a\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "1\n",
      "9\n",
      "9\n",
      "7\n",
      ",\n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "e\n",
      "n\n",
      "d\n",
      "e\n",
      "d\n",
      " \n",
      "u\n",
      "p\n",
      " \n",
      "u\n",
      "n\n",
      "c\n",
      "o\n",
      "v\n",
      "e\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      " \n",
      "f\n",
      "e\n",
      "m\n",
      "i\n",
      "n\n",
      "i\n",
      "s\n",
      "t\n",
      " \n",
      "h\n",
      "i\n",
      "s\n",
      "t\n",
      "o\n",
      "r\n",
      "y\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "i\n",
      "n\n",
      "d\n",
      "i\n",
      "a\n",
      "n\n",
      " \n",
      "n\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "b\n",
      "u\n",
      "i\n",
      "l\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "r\n",
      "a\n",
      "r\n",
      "e\n",
      "l\n",
      "y\n",
      " \n",
      "s\n",
      "e\n",
      "e\n",
      "n\n",
      ",\n",
      " \n",
      "n\n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      " \n",
      "h\n",
      "e\n",
      "a\n",
      "r\n",
      "d\n",
      ",\n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "l\n",
      "o\n",
      "v\n",
      "i\n",
      "n\n",
      "g\n",
      "l\n",
      "y\n",
      " \n",
      "t\n",
      "o\n",
      "l\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "c\n",
      "a\n",
      "m\n",
      "e\n",
      "r\n",
      "a\n",
      " \n",
      "c\n",
      "h\n",
      "r\n",
      "o\n",
      "n\n",
      "i\n",
      "c\n",
      "l\n",
      "e\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "h\n",
      "o\n",
      "m\n",
      "a\n",
      "i\n",
      " \n",
      "v\n",
      "y\n",
      "a\n",
      "r\n",
      "a\n",
      "w\n",
      "a\n",
      "l\n",
      "l\n",
      "a\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "review1=sentences[0].lower()\n",
    "for i in review1:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "544a4a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sabeena met Homai to shoot her documentary Three Women and a Camera in                        and ended up uncovering a feminist history of Indian nation building rarely seen  never heard                   and lovingly told in the Camera Chronicles of Homai Vyarawalla '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent=re.sub('[^a-zA-Z]',' ',sentences[0])\n",
    "test_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f6c8a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sabeena met homai shoot documentary three woman camera ended uncovering feminist history indian nation building rarely seen never heard lovingly told camera chronicle homai vyarawalla', 'homai lived quiet retired life year took success philosophical way took anonymity say sabeena eight year homai passing', 'film beginning long association homai', 'wrote letter would travel baroda every month look photograph try understand person behind image']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b5de3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 2, 1, 1, 0, 1, 0, 1, 0, 1, 1, 2, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 2,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 2],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 0, 0, 0, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#countvectorizer = a text preprocessing technique commonly used in (NLP) tasks for converting a collection of text documents into a numerical representation.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95e90d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 2 1 1 0 1 0 1 0 1 1 2 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0\n",
      "  1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 2 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1\n",
      "  1 1 0 0 1 0 0 2 0 0 0 0 0 1 0 0 0 2]\n",
      " [0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#threshold = an amount, level, or limit on a scale.\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70bbf232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3be8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
